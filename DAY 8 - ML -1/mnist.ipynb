{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:42.851063Z",
     "start_time": "2025-10-17T15:10:52.276002Z"
    }
   },
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.src.utils import to_categorical\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.234094Z",
     "start_time": "2025-10-17T15:11:42.896126Z"
    }
   },
   "cell_type": "code",
   "source": "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()",
   "id": "e8ea70cd671306b7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.648712Z",
     "start_time": "2025-10-17T15:11:43.254317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plt.imshow(train_images[0], cmap=plt.get_cmap('gray'))\n",
    "plt.imshow(train_images[6], cmap=plt.cm.binary)"
   ],
   "id": "3be77c770415147a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1936c042cf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG1hJREFUeJzt3QlsFOf5x/HHNraxjY8Y29jGxgUSQhvAVSm4lIQ6AkGIhAKhUkhSCSoEgkJUcNNEjhIILZJbItEIRKCqVChqOIrKIZBKS0wwSoqpMEUItSCMTDD1QaDy+sIH9vz1Dn+7LPcMu/vs8f1Io/XuzusZz47nt+/Mu89GWZZlCQAAARYd6AUCAGAQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFAxQIJMb2+v1NXVSXJyskRFRWmvDgDAIVPfoKWlRXJzcyU6Ojp0AsiET35+vvZqAACeUG1treTl5YVOAJmeT9+Kp6SkaK8OAMCh5uZmuyPRdzwPeABt2rRJPvroI2loaJDCwkLZuHGjTJw48ZHt+k67mfAhgAAgdD3qMopfBiHs3r1bSkpKZPXq1XL69Gk7gGbMmCHXrl3zx+IAACHILwG0fv16WbRokfz4xz+Wb33rW7JlyxZJTEyU3//+9/5YHAAgBPk8gLq6uqSqqkqmTZv2v4VER9v3T5w4cc/8nZ2d9vnCOycAQPjzeQBdv35denp6ZMiQIV6Pm/vmetDdysrKJDU1tX9iBBwARAb1D6KWlpaKx+Ppn8zoNwBA+PP5KLiMjAyJiYmRxsZGr8fN/ezs7Hvmj4+PtycAQGTxeQ8oLi5Oxo8fL+Xl5V7VDcz9SZMm+XpxAIAQ5ZfPAZkh2PPnz5fvfve79md/Pv74Y2lra7NHxQEA4LcAeu211+Trr7+WVatW2QMPvv3tb8vhw4fvGZgAAIhcUZapGhdEzDBsMxrODEigEgIAhJ7HPY6rj4IDAEQmAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAhEcAffjhhxIVFeU1jR492teLAQCEuAH++KXPPfecfPbZZ/9byAC/LAYAEML8kgwmcLKzs/3xqwEAYcIv14AuXrwoubm5MmLECHnzzTflypUrD5y3s7NTmpubvSYAQPjzeQAVFRXJtm3b5PDhw7J582apqamRF154QVpaWu47f1lZmaSmpvZP+fn5vl4lAEAQirIsy/LnApqamqSgoEDWr18vCxcuvG8PyEx9TA/IhJDH45GUlBR/rhoAwA/Mcdx0KB51HPf76IC0tDQZNWqUVFdX3/f5+Ph4ewIARBa/fw6otbVVLl26JDk5Of5eFAAgkgPo7bffloqKCrl8+bL8/e9/lzlz5khMTIy8/vrrvl4UACCE+fwU3NWrV+2wuXHjhmRmZsrzzz8vlZWV9s8AAPgtgHbt2uXrXwkACEPUggMAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqBugsFpHKsqyALCcqKspVu56eHsdtoqOjA7J+t27dctxmwIDg/hfv7e0NyPYOdt3d3QF7bd3+b/hD+L2SAICQQAABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQEVwVypE2AmmQoi+KpYaqL8p2AuLfvLJJ47brF271nGburo6CTexsbESiegBAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUBHc1Q2BABcIDeaCnzt27HDc5syZM66WtWfPHsdtBg4c6LhNZmam4zavv/664zY7d+6UYNbV1eW4zbp161wt6/3335dgQQ8IAKCCAAIAhEYAHT9+XGbNmiW5ubn2aY79+/ffc7pk1apVkpOTIwkJCTJt2jS5ePGiL9cZABCJAdTW1iaFhYWyadOmB56X3LBhg2zZskVOnjwpSUlJMmPGDOno6PDF+gIAwoTjK64zZ860p/sxvZ+PP/7Yvsj1yiuv2I9t375dhgwZYveU5s2b9+RrDAAICz69BlRTUyMNDQ32abc+qampUlRUJCdOnLhvm87OTmlubvaaAADhz6cBZMLHMD2eO5n7fc/drayszA6pvik/P9+XqwQACFLqo+BKS0vF4/H0T7W1tdqrBAAItQDKzs62bxsbG70eN/f7nrtbfHy8pKSkeE0AgPDn0wAaPny4HTTl5eX9j5lrOmY03KRJk3y5KABApI2Ca21tlerqaq+BB6bcR3p6ugwbNkxWrFgha9eulWeeecYOpA8++MD+zNDs2bN9ve4AgEgKoFOnTsmLL77Yf7+kpMS+nT9/vmzbtk3eeecd+7NCixcvlqamJnn++efl8OHDrupEAQDCV5TlptKjH5lTdmY0nBmQwPWg4BbIIqGB4qZqh5vCnQ/6WMLD/O1vf3PcZsSIEeJGRkaG4zbJycmO21y+fNlxm//85z+O25jjSTDbvn17QPY74+DBgxIsx3H1UXAAgMhEAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAAiNr2OASG9vr+M20dHOs76rq8txm7i4OAmUQFW2Nl/r4dR7773nalm7d+923CYpKclxm5ycHMdtJk6c6LhNd3e3uNHe3u64zejRowNS2dp8x1igXLt2LSD7UMn/f62NE+fPnxc3qqqqHLcZP368+AM9IACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACqiLMuyJIg0NzdLamqqeDweSUlJ8euy3P7pboqRxsTESLgpLy933ObPf/6z4zY7duxw3CY9PV3cyMvLc9xmwADnNX3N/h2IAqEJCQnihpsCq62trY7bZGVlBaRAaF1dnbjhZvuNHTvWcZsDBw44btPR0SFuBOJ1etzjOD0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKpxXUQwjUVFRrtoFc2HRDRs2OG6zefNmV8tqbGx03CY/P99xmzFjxgSkQKjbvylQ+56bNm4L7kZHO39vmpmZ6biNKVoZCN///vddtdu3b58Ewtq1ax232bRpk6tlFRQUOG7zxz/+0dH8LS0tjzUfPSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqwqYY6enTpx23OXLkiKtlXbhwwXGbjo4Ox23q6uoct3ncIoB3SktLEzfy8vIct/F4PI7bdHZ2BmQ5biUmJjpuc+vWrYAUFnVTVNTo7u4OyLISEhIctxk4cKDjNidPnhQ3cnJyHLdpa2tz3Gbo0KGO24waNUrcaG9vd9zmd7/7nV/+Z+kBAQBUEEAAgNAIoOPHj8usWbMkNzfX/n6S/fv3ez2/YMEC+/E7p5deesmX6wwAiMQAMuc3CwsLH/plSCZw6uvr+6edO3c+6XoCACJ9EMLMmTPt6WHi4+MlOzv7SdYLABDm/HIN6NixY5KVlSXPPvusLF26VG7cuPHQ0RLma3nvnAAA4c/nAWROv23fvl3Ky8vl17/+tVRUVNg9pp6envvOX1ZWJqmpqf1Tfn6+r1cJABAJnwOaN29e/89jx46VcePGyciRI+1e0dSpU++Zv7S0VEpKSvrvmx4QIQQA4c/vw7BHjBghGRkZUl1d/cDrRSkpKV4TACD8+T2Arl69al8DcvOJYgBA+HJ8Cq61tdWrN1NTUyNnzpyR9PR0e1qzZo3MnTvXHgV36dIleeedd+Tpp5+WGTNm+HrdAQCRFECnTp2SF198sf9+3/Wb+fPny+bNm+Xs2bPyhz/8QZqamuwPq06fPl1++ctf2qfaAABwHUDFxcUPLYr417/+VXzht7/9raNChXv37nW8jJs3b4obbopCxsXFBaQgZFJSUkD+nr7ecCAKVrop9um2wKqbIqGm2kcgitO6eZ3cFHI1ent7A/L/5GY7dHV1OW5jRti6ERMT47jNU0895bhNbGxswI5fbgoW+wu14AAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKqIst6WQ/cR8JbepXHvlyhVH3456/vx5x8v68ssvxY1z5845bvPVV185bvPf//43IBVy3VTdNgYMGBCQKss9PT2O21y/fl3ccFOt2836uano7OZ1CuS/96BBgwJSvd1NZXk3Va3dVjofOHBgQF6nNJcV39vb2x232bhxo6P529ra5OWXXxaPx/PQ4zg9IACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACqcV5MMEFOcz0mBvjFjxjheRlFRkQRKZ2en4zY1NTWO21RXVztuc/nyZXGjrq7OcZuOjo6AFGp0U/TUbTHSwYMHO26TnJwckOW4LVhpCgIHYlmJiYkBaeOWm6KxgSoAm5GR4aqdmwKwTouymqLSj4MeEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABVBW4zUFDZMSUl57Pnb2tocL6O+vl7cCFSxwfT0dMdtiouLA1Ig1IiNjZVA6Onp8XvxxCcpYupm+7n5m9wUxuzu7hY33PxNra2tjtt8/fXXjtu0tLQEbDu42cdv3brluE17e3tACtoaAwY4P+wXFBT4ZV+gBwQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEBF0BYjdSopKSkgbQLp5s2bASm6GBMTI264KT7Z2dkZsEKSbrgpEuqmgKnbbR6Iv8dtMVc3xTGHDh0akGLAbgqEBvL/qcfF6+R2H3Jz3MvNzXU0f3Nz82PNRw8IAKCCAAIABH8AlZWVyYQJE+yudlZWlsyePVsuXLhwz/eILFu2TAYPHiyDBg2SuXPnSmNjo6/XGwAQSQFUUVFhh0tlZaUcOXLEPj86ffp0ry+DW7lypRw8eFD27Nljz19XVyevvvqqP9YdABDCoqwn+HpP822GpidkgmbKlCni8XgkMzNTduzYIT/84Q/tec6fPy/f/OY35cSJE/K9733vsS5epaam2r/LyTeihqNADUJwe6Hazbc4MgghNAYhuLlo7+abNt38jzMIITQGITzOcfyJrgGZX37nV0dXVVXZL9i0adP65xk9erQMGzbMDqAHHZDMyt45AQDCn+sAMu/6VqxYIZMnT5YxY8bYjzU0NEhcXJykpaV5zTtkyBD7uQddVzJJ2Tfl5+e7XSUAQCQEkLkWdO7cOdm1a9cTrUBpaandk+qbamtrn+j3AQDC+IOoy5cvl0OHDsnx48clLy+v//Hs7Gzp6uqSpqYmr16QGQVnnruf+Ph4ewIARJZopxf/TPjs27dPjh49KsOHD/d6fvz48RIbGyvl5eX9j5lh2leuXJFJkyb5bq0BAJHVAzKn3cwItwMHDtifBeq7rmOu3SQkJNi3CxculJKSEntgghn98NZbb9nh8zgj4AAAkcNRAG3evNm+LS4u9np869atsmDBAvvn3/zmNxIdHW1/ANWMcJsxY4Z88sknvlxnAECkfw7IH/gcEACEtoB8DggAALcIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCAAQ/AFUVlYmEyZMkOTkZMnKypLZs2fLhQsXvOYpLi6WqKgor2nJkiW+Xm8AQCQFUEVFhSxbtkwqKyvlyJEj0t3dLdOnT5e2tjav+RYtWiT19fX907p163y93gCAEDfAycyHDx/2ur9t2za7J1RVVSVTpkzpfzwxMVGys7N9t5YAgLDzRNeAPB6PfZuenu71+KeffioZGRkyZswYKS0tlfb29gf+js7OTmlubvaaAADhz1EP6E69vb2yYsUKmTx5sh00fd544w0pKCiQ3NxcOXv2rLz77rv2daK9e/c+8LrSmjVr3K4GACBERVmWZblpuHTpUvnLX/4iX3zxheTl5T1wvqNHj8rUqVOlurpaRo4ced8ekJn6mB5Qfn6+3btKSUlxs2oAAEXmOJ6amvrI47irHtDy5cvl0KFDcvz48YeGj1FUVGTfPiiA4uPj7QkAEFkcBZDpLL311luyb98+OXbsmAwfPvyRbc6cOWPf5uTkuF9LAEBkB5AZgr1jxw45cOCA/VmghoYG+3HT1UpISJBLly7Zz7/88ssyePBg+xrQypUr7RFy48aN89ffAAAI92tA5kOl97N161ZZsGCB1NbWyo9+9CM5d+6c/dkgcy1nzpw58v777z/29ZzHPXcIAIiga0CPyioTOObDqgAAPAq14AAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgZIkLEsy75tbm7WXhUAgAt9x+++43nIBFBLS4t9m5+fr70qAIAnPJ6npqY+8Pko61ERFWC9vb1SV1cnycnJEhUVdU+qmmCqra2VlJQUiVRsh9vYDrexHW5jOwTPdjCxYsInNzdXoqOjQ6cHZFY2Ly/vofOYjRrJO1gftsNtbIfb2A63sR2CYzs8rOfTh0EIAAAVBBAAQEVIBVB8fLysXr3avo1kbIfb2A63sR1uYzuE3nYIukEIAIDIEFI9IABA+CCAAAAqCCAAgAoCCACgImQCaNOmTfKNb3xDBg4cKEVFRfKPf/xDIs2HH35oV4e4cxo9erSEu+PHj8usWbPsT1Wbv3n//v1ez5txNKtWrZKcnBxJSEiQadOmycWLFyXStsOCBQvu2T9eeuklCSdlZWUyYcIEu1JKVlaWzJ49Wy5cuOA1T0dHhyxbtkwGDx4sgwYNkrlz50pjY6NE2nYoLi6+Z39YsmSJBJOQCKDdu3dLSUmJPbTw9OnTUlhYKDNmzJBr165JpHnuueekvr6+f/riiy8k3LW1tdmvuXkTcj/r1q2TDRs2yJYtW+TkyZOSlJRk7x/mQBRJ28EwgXPn/rFz504JJxUVFXa4VFZWypEjR6S7u1umT59ub5s+K1eulIMHD8qePXvs+U1pr1dffVUibTsYixYt8tofzP9KULFCwMSJE61ly5b13+/p6bFyc3OtsrIyK5KsXr3aKiwstCKZ2WX37dvXf7+3t9fKzs62Pvroo/7HmpqarPj4eGvnzp1WpGwHY/78+dYrr7xiRZJr167Z26KioqL/tY+NjbX27NnTP8+///1ve54TJ05YkbIdjB/84AfWT3/6UyuYBX0PqKurS6qqquzTKnfWizP3T5w4IZHGnFoyp2BGjBghb775ply5ckUiWU1NjTQ0NHjtH6YGlTlNG4n7x7Fjx+xTMs8++6wsXbpUbty4IeHM4/HYt+np6fatOVaY3sCd+4M5TT1s2LCw3h88d22HPp9++qlkZGTImDFjpLS0VNrb2yWYBF0x0rtdv35denp6ZMiQIV6Pm/vnz5+XSGIOqtu2bbMPLqY7vWbNGnnhhRfk3Llz9rngSGTCx7jf/tH3XKQwp9/Mqabhw4fLpUuX5L333pOZM2faB96YmBgJN6Zy/ooVK2Ty5Mn2AdYwr3lcXJykpaVFzP7Qe5/tYLzxxhtSUFBgv2E9e/asvPvuu/Z1or1790qwCPoAwv+Yg0mfcePG2YFkdrA//elPsnDhQtV1g7558+b1/zx27Fh7Hxk5cqTdK5o6daqEG3MNxLz5ioTroG62w+LFi732BzNIx+wH5s2J2S+CQdCfgjPdR/Pu7e5RLOZ+dna2RDLzLm/UqFFSXV0tkapvH2D/uJc5TWv+f8Jx/1i+fLkcOnRIPv/8c6+vbzGvuTlt39TUFBH7w/IHbIf7MW9YjWDaH4I+gEx3evz48VJeXu7V5TT3J02aJJGstbXVfjdj3tlEKnO6yRxY7tw/zBdymdFwkb5/XL161b4GFE77hxl/YQ66+/btk6NHj9qv/53MsSI2NtZrfzCnncy10nDaH6xHbIf7OXPmjH0bVPuDFQJ27dplj2ratm2b9a9//ctavHixlZaWZjU0NFiR5Gc/+5l17Ngxq6amxvryyy+tadOmWRkZGfYImHDW0tJi/fOf/7Qns8uuX7/e/vmrr76yn//Vr35l7w8HDhywzp49a48EGz58uHXz5k0rUraDee7tt9+2R3qZ/eOzzz6zvvOd71jPPPOM1dHRYYWLpUuXWqmpqfb/QX19ff/U3t7eP8+SJUusYcOGWUePHrVOnTplTZo0yZ7CydJHbIfq6mrrF7/4hf33m/3B/G+MGDHCmjJlihVMQiKAjI0bN9o7VVxcnD0su7Ky0oo0r732mpWTk2Nvg6FDh9r3zY4W7j7//HP7gHv3ZIYd9w3F/uCDD6whQ4bYb1SmTp1qXbhwwYqk7WAOPNOnT7cyMzPtYcgFBQXWokWLwu5N2v3+fjNt3bq1fx7zxuMnP/mJ9dRTT1mJiYnWnDlz7INzJG2HK1eu2GGTnp5u/088/fTT1s9//nPL4/FYwYSvYwAAqAj6a0AAgPBEAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABANPwftbIFfY9poJQAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.664481Z",
     "start_time": "2025-10-17T15:11:43.662134Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_labels[6])",
   "id": "40d54bad6fdc1d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.677196Z",
     "start_time": "2025-10-17T15:11:43.673395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels_one_hot = to_categorical(train_labels, num_classes=10)\n",
    "test_labels_one_hot = to_categorical(test_labels, num_classes=10)"
   ],
   "id": "7141b4daae091794",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.687360Z",
     "start_time": "2025-10-17T15:11:43.684452Z"
    }
   },
   "cell_type": "code",
   "source": "train_labels_one_hot[0].shape",
   "id": "65ba53d4fe06ebdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.698424Z",
     "start_time": "2025-10-17T15:11:43.695687Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_labels_one_hot[0])",
   "id": "d2be39643336d5c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:43.710589Z",
     "start_time": "2025-10-17T15:11:43.707730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#THEREFORE\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Training labels shape:\", train_labels_one_hot.shape)\n",
    "print(\"Test images label shape:\", test_labels_one_hot.shape)"
   ],
   "id": "28946d6a643fd440",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (60000, 28, 28)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Training labels shape: (60000, 10)\n",
      "Test images label shape: (10000, 10)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BASIC ANN",
   "id": "8c651d21fad4a72e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:13:29.652981Z",
     "start_time": "2025-10-17T15:13:29.623721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ann_model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ann_model.summary()\n",
    "\n"
   ],
   "id": "558e5c10797763b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m784\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m100,480\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m8,256\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │           \u001B[38;5;34m650\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m109,386\u001B[0m (427.29 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m109,386\u001B[0m (427.29 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🎨 The Sequential Model: A Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Is This Model?\n",
    "\n",
    "This is a **sequential model**, which means it's a linear stack of layers where data flows in a straight line from one layer to the next. It's often used for building simple neural network architectures.\n",
    "\n",
    "Let's break down each layer:\n",
    "\n",
    "### 🧩 Layers Explained\n",
    "\n",
    "#### **1. `keras.layers.Flatten(input_shape=(28, 28, 1))`**\n",
    "This is the **input layer**. It takes a 2D image (28x28 pixels) and **\"flattens\"** it into a single 1D array of 784 pixels.\n",
    "- `input_shape=(28, 28, 1)`: Specifies the dimensions of the input. The `1` indicates that the images are grayscale (one color channel).\n",
    "\n",
    "#### **2. `keras.layers.Dense(128, activation='relu')`**\n",
    "This is the **first hidden layer**. It's a fully connected layer where every neuron receives input from all the neurons in the previous layer.\n",
    "- `128`: The number of neurons in this layer.\n",
    "- `activation='relu'`: The **Rectified Linear Unit (ReLU)** activation function. It introduces non-linearity, helping the network learn complex patterns by turning all negative values into zero.\n",
    "\n",
    "#### **3. `keras.layers.Dense(64, activation='relu')`**\n",
    "This is the **second hidden layer**. It's another fully connected layer, but with 64 neurons. It works similarly to the first hidden layer, processing the features passed on from it.\n",
    "\n",
    "#### **4. `keras.layers.Dense(10, activation='softmax')`**\n",
    "This is the **output layer**.\n",
    "- `10`: The number of neurons, which matches the number of classes you're trying to predict (e.g., the 10 classes in Fashion MNIST).\n",
    "- `activation='softmax'`: This activation function converts the raw output values into a **probability distribution**. The result is 10 values, where each value is the probability that the input belongs to a specific class. The sum of all these probabilities will equal `1`."
   ],
   "id": "c526703c616e03ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🖼️ Understanding Image Dimensions & Model Complexity\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Why `(28, 28, 1)`?\n",
    "\n",
    "The **`(28, 28, 1)`** in your **`Flatten`** layer's **`input_shape`** refers to the dimensions of each input image.\n",
    "\n",
    "- **`28`**: This is the **height** of the image in pixels.\n",
    "- **`28`**: This is the **width** of the image in pixels.\n",
    "- **`1`**: This is the number of **color channels**. Since the Fashion MNIST images are grayscale (black and white), they only have one channel. For a color image (like a regular photograph), this would typically be **`3`** for Red, Green, and Blue (**RGB**) channels.\n",
    "\n",
    "Even though your **`train_images`** array might look like it has a shape of `(60000, 28, 28)`, Keras expects the channel dimension to be explicitly stated for certain layers, especially when the data is being preprocessed. The **`Flatten`** layer takes this **`(28, 28, 1)`** input and converts it into a single, flat array of **784** numbers (`28 * 28 * 1`).\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Can I Decrease the Hidden Layers?\n",
    "\n",
    "Yes, you can absolutely decrease the number of hidden layers or the number of neurons within them. This process is known as **hyperparameter tuning**, and it's a core part of building neural networks.\n",
    "\n",
    "- **Fewer Layers/Neurons**: A simpler model with fewer layers will have fewer parameters to learn. This can make the model **faster to train** and **less prone to overfitting** (when the model learns the training data too well and performs poorly on new data). However, a model that is too simple might not be powerful enough to capture the complexity of the data, leading to **underfitting** and poor performance.\n",
    "\n",
    "- **More Layers/Neurons**: A more complex model can learn more intricate patterns. This can potentially lead to **higher accuracy**, but it also increases the risk of overfitting and requires more computational resources and time to train.\n",
    "\n",
    "You can try modifying your model to be simpler, like this:\n",
    "\n",
    "```python\n",
    "ann_model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ],
   "id": "7006c73de7bec3b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Key Components of a Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Softmax (Activation Function)\n",
    "\n",
    "### **What is it?**\n",
    "**Softmax** is a special activation function used in the **final layer** of a neural network for multi-class classification problems. It takes the raw outputs (called **logits**) from the last layer and converts them into a set of **probabilities**.\n",
    "\n",
    "### **Role and Importance**\n",
    "Its main job is to ensure that the model's output is a **probability distribution**. The probabilities for all possible classes add up to **1**, and the highest probability corresponds to the class that the model believes is the most likely prediction. This makes the output easily interpretable. For example, the model might output `[0.05, 0.05, 0.85, 0.05, ...]` which clearly indicates an **85% probability** that the input belongs to the third class.\n",
    "\n",
    "### **Other Examples**\n",
    "If you were doing a **binary classification** (e.g., is this a cat or not a cat?), you would typically use a **sigmoid** activation function in the output layer. A sigmoid function outputs a single value between **0** and **1**, representing the probability of the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Loss (Loss Function)\n",
    "\n",
    "### **What is it?**\n",
    "The **loss function**, also known as the cost function, is a measure of how well your model's predictions align with the actual, correct labels. It calculates the **\"penalty\"** for every incorrect prediction. A higher loss value means the model is performing poorly, while a lower loss value indicates good performance.\n",
    "\n",
    "### **Role and Importance**\n",
    "The loss function is the target that the model tries to **minimize** during training. It provides the feedback signal that the optimizer uses to adjust the model's weights. Without a loss function, the model would have no way of knowing if it's getting better or worse.\n",
    "\n",
    "### **Other Examples and Their Impacts**\n",
    "- **`categorical_crossentropy`**: This is the most common loss function for **multi-class classification** problems, especially when your labels are **one-hot encoded**.\n",
    "- **`sparse_categorical_crossentropy`**: This is an alternative that works directly with **integer labels** (e.g., `0, 1, 2`) instead of one-hot encoded labels, saving you a step in data preparation.\n",
    "- **`binary_crossentropy`**: This is used for **binary classification** problems and is typically paired with the sigmoid activation function.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Optimizer (Optimization Algorithm)\n",
    "\n",
    "### **What is it?**\n",
    "An **optimizer** is the algorithm that adjusts the weights of your neural network during the training process to minimize the loss function. Think of it as the **\"engine\"** of the learning process.\n",
    "\n",
    "### **Role and Importance**\n",
    "The optimizer's job is to figure out how to best update the model's internal parameters (weights and biases) to make the model's predictions more accurate. It analyzes the loss value and intelligently decides the size and direction of the adjustments.\n",
    "\n",
    "### **Other Examples and Their Impacts**\n",
    "- **`adam`**: You're using **`adam`**, which is one of the most popular and generally effective optimizers. It adapts the learning rate for each weight, making it highly efficient.\n",
    "- **`sgd`**: **Stochastic Gradient Descent** is a foundational optimizer. It's simpler and can be slower to train, but it can sometimes find a better solution if carefully tuned.\n",
    "- **`rmsprop`**: This is another effective optimizer that adjusts the learning rate for each parameter and is often a strong alternative to **`adam`**.\n",
    "\n",
    "The choice of optimizer can significantly impact how quickly your model learns and the final accuracy it achieves. For most problems, **`adam`** is a great starting point, but trying others can sometimes yield a slight performance boost."
   ],
   "id": "c592b7e9435693bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:11:49.076303Z",
     "start_time": "2025-10-17T15:11:43.856001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ann_model.fit(\n",
    "    train_images,\n",
    "    train_labels_one_hot,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(test_images, test_labels_one_hot)\n",
    ")"
   ],
   "id": "74177eebbdfda938",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m469/469\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.7428 - loss: 3.5095 - val_accuracy: 0.7697 - val_loss: 1.1360\n",
      "Epoch 2/5\n",
      "\u001B[1m469/469\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.7988 - loss: 0.7845 - val_accuracy: 0.7851 - val_loss: 0.7623\n",
      "Epoch 3/5\n",
      "\u001B[1m469/469\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8194 - loss: 0.5627 - val_accuracy: 0.8173 - val_loss: 0.5846\n",
      "Epoch 4/5\n",
      "\u001B[1m469/469\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8367 - loss: 0.4812 - val_accuracy: 0.8258 - val_loss: 0.5190\n",
      "Epoch 5/5\n",
      "\u001B[1m469/469\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8486 - loss: 0.4332 - val_accuracy: 0.8386 - val_loss: 0.4814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1936c180830>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
